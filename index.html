<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MimicTalk: Few-shot 3D Talking Portrait Synthesis">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MimicTalk: Mimicking a personalized and expressive 3D talking face in few minutes</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
<style> .table-warped {overflow:scroll;}</style>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title is-1 publication-title">
          MimicTalk: Mimicking a personalized and expressive 3D talking face in few minutes</h2>
        <div class="is-size-5 publication-authors">
          <span class="author-block">
            Anonymous Authors</span>
        </div>
        <div class="column has-text-centered">
          <div class="publication-links">
            <!-- Code Link. -->
            <span class="link-block">
              <a href="" class="external-link button is-normal is-rounded is-dark" onclick="alert('We plan to release the source code after the rebuttal phase.')">
              <span class="icon">
              <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
              </span>
              <span>Code</span>
              </a>
              </span>
          </div>
        </div>
      </div>  
    </div>
    
    <!-- Rebuttal Demos. -->

    <div class="columns is-centered has-text-centered">
      <div class="column is-centered">
        <div class="content has-text-centered is-centered">
          <h3 class="title is-4">Rebuttal Demo 1. More Identities</h3>
          This demo compares our method with the baseline of <u><b>9 additional identities</b></u>, (all training videos are 10-second-long), and the results prove that our method has better data efficiency and lip-sync quality.<br></br>
          Please focus on the lip motion, the talking face avatars are driving by the audio track in the video.
          <p>â€¢ Please focus on the lip motion, the talking face avatars are driving by the audio track in the video.</p>
          <video controls preload width="50%">
            <source src="static/videos/rebuttal/more_ID.mp4" type="video/mp4"> 
          </video>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-centered">
        <div class="content has-text-centered is-centered">
          <h3 class="title is-4">Rebuttal Demo 2. Audio-to-Pose</h3>
          To <u><b>predict head pose from audio</b></u>, we additionally train an audio-to-pose model, which follows the main structure of the ICS-A2M model proposed in the original paper. The demo show that our model can produce novel head poses that are coherent with the input audio.<br></br>
          <video controls preload width="50%">
            <source src="static/videos/rebuttal/audio2pose.mp4" type="video/mp4"> 
          </video>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-centered">
        <div class="content has-text-centered is-centered">
          <h3 class="title is-4">Rebuttal Demo 3. Driven by OOD poses</h3>
          We additionally compare our method with the baseline when driven by <u><b>various OOD head poses</b></u>, and the results show that our method could well handle the OOD pose while the baseline cannot.<br></br>
          <video controls preload width="50%">
            <source src="static/videos/rebuttal/ood_pose.mp4" type="video/mp4"> 
          </video>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-centered">
        <div class="content has-text-centered is-centered">
          <h3 class="title is-4">Rebuttal Demo 4. Talking Style Mimicking</h3>
          We additionally introduce <u><b>classifier-free guidance (CFG)</b></u> into the ICS-A2M model, which further improves the style similarity for our flow-matching-based motion generation. The results show that our method could well handle <u><b>various style references (6 prompts in the video)</b></u>, while the baseline degrades in identity similarity.<br></br>
          <video controls preload width="50%">
            <source src="static/videos/rebuttal/more_style.mp4" type="video/mp4"> 
          </video>
        </div>
      </div>
    </div>

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">

      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Talking face generation (TFG) aims to animate a target identity's face to create realistic talking videos. Personalized TFG is a variant which emphasizes the perceptual identity similarity between the synthesized result to the real person (both from the perspective of static appearance and dynamic talking style). While previous works typically solve this problem by learning an individual neural radiance field (NeRF) for each identity to implicitly store the identity's static and dynamic information, we find it inefficient and non-generalized due to the per-identity-per-training framework and the limited training data. To this end, we propose MimicTalk, the first attempt that exploit the rich knowledge from a NeRF-based person-agnostic genric model for improving the effieicency and robustness of personalized TFG. To be specific, (1) we first come up with a person-agnostic 3D TFG model as the base model, and propose to adapt it into a specific identity; (2) we propose a static-dynamic hybrid adaptation pipeline to help the model to learn the personalized static appearance and facial dynamic; (3) we propose an in-context stylized audio-to-motion (ICS-A2M) model that enables generating co-speech facial motion while mimicking the talking style of the target person. The adaptation process to an unseen identity can be converged in only few minutes. Experiments show that our MimicTalk surpasses previous baselines in terms of video quality, efficiency, and expresiveness. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Overall Framework -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overall Framework</h2>
        <div class="content has-text-justified">
          <p>
            The overall inference process of MimicTalk is demonstrated as follows:
          </p>
          <p>
            <img src="./static/images/inference.png"
      class="interpolation-image"
      alt="The inference process of MimicTalk."/>
          </p>
        </div>
      </div>
    </div>
    <!--/ Overall Framework -->
  </div>
<!-- </section> -->
  <div> </div>





    <div class="columns is-centered has-text-centered">
      <div class="column is-centered">
        <div class="content has-text-centered is-centered">
          <h3 class="title is-4">1. High-quality Personalized Talking Face Generation</h3>
          <h4 class="title is-6">The models are trained through the proposed SD-Hybrid adaptation pipeline, talking 1000 iterations and less than 10 minutes for training.</h4>
          <video controls preload width="20%">
            <source src="static/videos/demo_sd_hybrid.mp4" type="video/mp4"> 
          </video>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-centered">
        <div class="content has-text-centered is-centered">
          <h3 class="title is-4">2. Comparison with Person-Dependent NeRF-based Baselines</h3>
          <video controls preload width="40%">
            <source src="static/videos/comparison_with_nerf_audio_driven.mp4" type="video/mp4"> 
          </video>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-centered">
        <div class="content has-text-centered is-centered">
          <h3 class="title is-4">3. Stylized and Expressive Co-Speech Motion Generation with the ICS-A2M model</h3>
          <video controls preload width="40%">
            <source src="static/videos/demo_ics_a2m.mp4" type="video/mp4"> 
          </video>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-centered">
        <div class="content has-text-centered is-centered">
          <h3 class="title is-4">4. Comparison with StyleTalk for Talking Style Controll</h3>
          <video controls preload width="40%">
            <source src="static/videos/comparison_with_styletalk.mp4" type="video/mp4"> 
          </video>
        </div>
      </div>
    </div>


  </div>
</section>



</body>
</html>
